import os
import json
import base64
from pathlib import Path
from PIL import Image
from langchain_openai import ChatOpenAI
import google.generativeai as genai
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from typing import Dict, Any, Tuple
import re

class LuggageVolumeEstimator:
    def __init__(self):
        # API í‚¤ ë¡œë“œ
        secrets_path = Path(__file__).parent.parent / 'tetris_secrets.json'
        with open(secrets_path, 'r', encoding='utf-8') as f:
            secrets = json.load(f)
        
        self.openai_api_key = secrets['openai']['OPENAI_API_KEY']
        self.google_api_key = secrets['google']['GOOGLE_API_KEY']
        
        # Google Gemini API í‚¤ ì„¤ì •
        genai.configure(api_key=self.google_api_key)
        
        # ëª¨ë¸ ëª©ë¡
        self.models = ["gpt-4o", "gpt-4.1", "gemini-2.5-flash", "gemini-2.5-pro"]
        
        # ê²½ë¡œ ì„¤ì •
        self.base_path = Path(__file__).parent
        self.image_base_path = self.base_path / "chain1_image"
        self.prompt_path = self.base_path / "chain1_prompt"
        self.output_base = self.base_path / "chain1_out" / "chat2" / "zero"
        
        # ì‹œë‚˜ë¦¬ì˜¤ í´ë” ëª©ë¡
        self.scenarios = ["items_combined", "items_separated"]
        
        # í”„ë¡¬í”„íŠ¸ íŒŒì¼ ë¡œë“œ
        self.zero_chat1_prompt = self._load_prompt("zero_chat1.txt")
        self.zero_chat2_prompt = self._load_prompt("zero_chat2.txt")
        
        # ë¶€í”¼ ì°¸ì¡° ê°ì²´ë“¤ (ì´ë¦„ë§Œ)
        self.volume_references = {
            1: "ì¶•êµ¬ê³µ",
            2: "2L ìƒìˆ˜ 6ê°œ ë¬¶ìŒ",
            3: "ê¸°ë‚´ìš© ìºë¦¬ì–´",
            4: "ì¤‘í˜• ìºë¦¬ì–´", 
            5: "ëŒ€í˜• ìºë¦¬ì–´",
            6: "ë“œëŸ¼ ì„¸íƒê¸°"
        }
        
        # ë­ì²´ì¸ ì²´ì¸ ì„¤ì •
        self._setup_chains()

    def _load_prompt(self, filename: str) -> str:
        """í”„ë¡¬í”„íŠ¸ íŒŒì¼ ë¡œë“œ"""
        prompt_file = self.prompt_path / filename
        if not prompt_file.exists():
            raise FileNotFoundError(f"í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {prompt_file}")
        
        with open(prompt_file, 'r', encoding='utf-8') as f:
            return f.read().strip()

    def _setup_chains(self):
        """ë­ì²´ì¸ ì²´ì¸ ì„¤ì •"""
        # OpenAI ì²´ì¸ìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.openai_prompt_template = ChatPromptTemplate.from_messages([
            ("system", "You are an expert in luggage volume estimation and analysis."),
            ("human", [
                {"type": "text", "text": "{prompt_text}"},
                {"type": "image_url", "image_url": {"url": "data:image/jpeg;base64,{image_data}"}}
            ])
        ])
        
        # ì¶œë ¥ íŒŒì„œ
        self.output_parser = StrOutputParser()

    def _create_openai_chain(self, model_name: str):
        """OpenAI ì²´ì¸ ìƒì„±"""
        llm = ChatOpenAI(
            model=model_name,
            api_key=self.openai_api_key,
            temperature=0.7,
            max_tokens=10000
        )
        return self.openai_prompt_template | llm | self.output_parser

    def _create_gemini_model(self, model_name: str):
        """Gemini ëª¨ë¸ ìƒì„±"""
        return genai.GenerativeModel(
            model_name,
            system_instruction="You are an expert in luggage volume estimation and analysis.",
            generation_config=genai.types.GenerationConfig(temperature=0.3)
        )

    def encode_image(self, image_path: Path) -> str:
        """ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©"""
        with open(image_path, "rb") as image_file:
            image_data = image_file.read()
        
        image = Image.open(image_path)
        print(f"ğŸ“· ì´ë¯¸ì§€ í¬ê¸°: {image.size}")
        return base64.b64encode(image_data).decode('utf-8')

    def call_llm_with_history(self, model_name: str, image_path: Path, messages: list) -> str:
        """LLM í˜¸ì¶œ (ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬í•¨)"""
        try:
            if model_name.startswith("gpt"):
                # OpenAI ëª¨ë¸ - ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ ì§ì ‘ ì „ë‹¬
                llm = ChatOpenAI(
                    model=model_name,
                    api_key=self.openai_api_key,
                    temperature=0.7,
                    max_tokens=10000
                )
                
                response = llm.invoke(messages)
                return response.content
                
            else:
                # Gemini ëª¨ë¸ - íˆìŠ¤í† ë¦¬ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ì „ë‹¬
                image = Image.open(image_path)
                
                # ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                history_text = ""
                current_prompt = ""
                
                for msg in messages:
                    if hasattr(msg, 'content'):
                        if isinstance(msg.content, list):
                            # ì´ë¯¸ì§€ê°€ í¬í•¨ëœ ë©”ì‹œì§€ ì²˜ë¦¬
                            for part in msg.content:
                                if part.get('type') == 'text':
                                    if msg.__class__.__name__ == 'HumanMessage':
                                        current_prompt = part['text']
                                    elif msg.__class__.__name__ == 'AIMessage':
                                        history_text += f"Human: {current_prompt}\n\nAssistant: {part['text']}\n\n"
                        else:
                            # í…ìŠ¤íŠ¸ë§Œ ìˆëŠ” ë©”ì‹œì§€
                            if msg.__class__.__name__ == 'HumanMessage':
                                current_prompt = msg.content
                            elif msg.__class__.__name__ == 'AIMessage':
                                history_text += f"Human: {current_prompt}\n\nAssistant: {msg.content}\n\n"
                
                # í˜„ì¬ í”„ë¡¬í”„íŠ¸ë¥¼ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
                final_prompt = history_text + f"Human: {current_prompt}"
                
                model = self._create_gemini_model(model_name)
                response = model.generate_content([
                    {"role": "user", "parts": [{"text": final_prompt}, image]}
                ])
                return response.text
                
        except Exception as e:
            print(f"âš ï¸ {model_name} í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return None

    def get_user_volume_choice(self, luggage_description: str) -> Tuple[int, str]:
        """ì‚¬ìš©ìë¡œë¶€í„° ë¶€í”¼ ì„ íƒ ë°›ê¸°"""
        print("\në‹¤ìŒ ì¤‘ì—ì„œ í•´ë‹¹ ì§ê³¼ ë¶€í”¼ê°€ ê°€ì¥ ìœ ì‚¬í•œ í•­ëª©ì„ ì„ íƒí•˜ì„¸ìš”:")
        print()
        
        # í›„ë³´ ë¦¬ìŠ¤íŠ¸ í‘œì‹œ
        for i, name in self.volume_references.items():
            print(f"{i}. {name}")
        
        while True:
            try:
                choice = int(input("\nì„ íƒí•˜ì„¸ìš” (1-5): "))
                if choice in self.volume_references:
                    selected_name = self.volume_references[choice]
                    print(f"\nâœ¨ ì„ íƒë¨: {choice}ë²ˆ - {selected_name}")
                    
                    # ì‚¬ìš©ì ì œê³µ ì •ë³´ í…ìŠ¤íŠ¸ ìƒì„±
                    candidate_list = [f"{i}. {name}" for i, name in self.volume_references.items()]
                    candidate_list_text = "  ".join(candidate_list)
                    user_info_text = f"""í›„ë³´ ë¦¬ìŠ¤íŠ¸:
{candidate_list_text}
ì‘ë‹µ: {choice}ë²ˆ ({selected_name})"""
                    
                    return choice, user_info_text
                else:
                    print("âš ï¸ 1-5 ì‚¬ì´ì˜ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            except ValueError:
                print("âš ï¸ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    def create_reasoning_prompt(self, user_choice: int) -> str:
        """Reasoning Extraction í”„ë¡¬í”„íŠ¸ ìƒì„± (ì‚¬ìš©ì ì •ë³´ + zero_chat2.txtë§Œ)"""
        selected_name = self.volume_references[user_choice]
        
        # í›„ë³´ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        candidate_list = [f"{i}. {name}" for i, name in self.volume_references.items()]
        candidate_list_text = "  ".join(candidate_list)
        
        user_info_section = f"""[ì •ë³´ ì œê³µ] ë‹¤ìŒ í›„ë³´ ì¤‘ í•´ë‹¹ ì§ê³¼ ë¶€í”¼ê°€ ê°€ì¥ ìœ ì‚¬í•œ í•­ëª©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

í›„ë³´ ë¦¬ìŠ¤íŠ¸:
{candidate_list_text}
ì‘ë‹µ: {user_choice}ë²ˆ ({selected_name})

"""
        
        return user_info_section + self.zero_chat2_prompt
    
    def create_answer_prompt(self, reasoning_prompt: str, reasoning_response: str) -> str:
        """Answer Extraction í”„ë¡¬í”„íŠ¸ ìƒì„± (Reasoning + ì‘ë‹µ + "ê·¸ëŸ¬ë¯€ë¡œ ë‹µì€:")"""
        return f"""{reasoning_prompt}

{reasoning_response}

ê·¸ëŸ¬ë¯€ë¡œ ë‹µì€: (ì „ì²´ ë¶€í”¼ë¥¼ ë¦¬í„° ë‹¨ìœ„ ìˆ«ìë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì˜ˆ: 94)"""

    def save_final_results(self, model_name: str, scenario: str, image_name: str, 
                          step1_response: str, user_info: str, 
                          reasoning_response: str, final_response: str):
        """ìµœì¢… ê²°ê³¼ íŒŒì¼ ì €ì¥"""
        output_dir = self.output_base / model_name / scenario
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ì„¹ì…˜ í—¤ë”ë§Œ í•˜ë“œì½”ë”©, ë‚´ìš©ì€ ë™ì 
        final_content = f"""[ì„ íƒëœ ì§]
{step1_response}

[ì‚¬ìš©ì ì œê³µ ì •ë³´]
{user_info}

[ì „ì²´ ë¶€í”¼ ì¶”ë¡  ê³¼ì •]
{reasoning_response}

[ìµœì¢… ì¶”ì • ì „ì²´ ë¶€í”¼]
{final_response}"""
        
        output_path = output_dir / f"{image_name}.txt"
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(final_content)
        print(f"ğŸ“ ìµœì¢… ê²°ê³¼ ì €ì¥: {output_path}")

    def process_single_image(self, model_name: str, scenario: str, image_name: str):
        """ë‹¨ì¼ ì´ë¯¸ì§€ ì²˜ë¦¬"""
        print(f"\n{'='*80}")
        print(f"ì²˜ë¦¬ ì¤‘: {model_name} | {scenario} | {image_name}")
        print(f"{'='*80}")
        
        image_path = self.image_base_path / scenario / f"{image_name}.jpeg"
        
        if not image_path.exists():
            print(f"âŒ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
            return
        
        base64_image = self.encode_image(image_path)
        
        # Step 1: zero_chat1.txt í”„ë¡¬í”„íŠ¸ë¡œ ì§ ì„ íƒ
        print("\n=== Step 1: ë¶€í”¼ ë¶„ì„í•˜ê¸° ì‰¬ìš´ ì§ ì„ íƒ ===")
        
        # ì´ˆê¸° ë©”ì‹œì§€ êµ¬ì„± (ì´ë¯¸ì§€ + zero_chat1 í”„ë¡¬í”„íŠ¸)
        system_msg = SystemMessage(content="You are an expert in luggage volume estimation and analysis.")
        step1_user_msg = HumanMessage(content=[
            {"type": "text", "text": self.zero_chat1_prompt},
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}}
        ])
        
        step1_messages = [system_msg, step1_user_msg]
        step1_response = self.call_llm_with_history(model_name, image_path, step1_messages)
        
        if not step1_response:
            print("âŒ Step 1 ì‹¤íŒ¨")
            return
        
        print("ğŸš€ LLM ì‘ë‹µ:")
        print(step1_response)
        
        # Step 2: ì‚¬ìš©ì ë¶€í”¼ ì„ íƒ
        print("\n=== Step 2: ì‚¬ìš©ì ë¶€í”¼ ì„ íƒ ===")
        user_choice, user_info_text = self.get_user_volume_choice(step1_response)
        
        # Step 3: Reasoning Extraction (íˆìŠ¤í† ë¦¬ + ì‚¬ìš©ì ì •ë³´ + zero_chat2.txt)
        print("\n=== Step 3: Reasoning Extraction ===")
        
        # íˆìŠ¤í† ë¦¬ì— Step 1 ì‘ë‹µ ì¶”ê°€
        step1_ai_msg = AIMessage(content=step1_response)
        
        # Reasoning Extraction í”„ë¡¬í”„íŠ¸ ìƒì„± (ì‚¬ìš©ì ì •ë³´ + zero_chat2.txtë§Œ)
        reasoning_prompt = self.create_reasoning_prompt(user_choice)
        step3_user_msg = HumanMessage(content=[
            {"type": "text", "text": reasoning_prompt},
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}}
        ])
        
        # íˆìŠ¤í† ë¦¬ í¬í•¨í•œ ë©”ì‹œì§€ êµ¬ì„±
        step3_messages = [system_msg, step1_user_msg, step1_ai_msg, step3_user_msg]
        reasoning_response = self.call_llm_with_history(model_name, image_path, step3_messages)
        
        if not reasoning_response:
            print("âŒ Step 3 (Reasoning Extraction) ì‹¤íŒ¨")
            return
        
        print("ğŸ” ì¶”ë¡  ê³¼ì •:")
        print(reasoning_response)
        
        # Step 4: Answer Extraction (ì „ì²´ íˆìŠ¤í† ë¦¬ + Reasoning Extraction + Step3ì‘ë‹µ + "ê·¸ëŸ¬ë¯€ë¡œ ë‹µì€:")
        print("\n=== Step 4: Answer Extraction ===")
        
        # Answer Extraction í”„ë¡¬í”„íŠ¸ = Reasoning Extraction + Step3ì‘ë‹µ + "ê·¸ëŸ¬ë¯€ë¡œ ë‹µì€:"
        answer_prompt = f"""{reasoning_prompt}

{reasoning_response}

ê·¸ëŸ¬ë¯€ë¡œ ë‹µì€:"""
        
        step4_user_msg = HumanMessage(content=[
            {"type": "text", "text": answer_prompt},
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}}
        ])
        
        # íˆìŠ¤í† ë¦¬ëŠ” ì´ë¯¸ì§€ + zero_chat1 + Step1ì‘ë‹µë§Œ
        step4_messages = [system_msg, step1_user_msg, step1_ai_msg, step4_user_msg]
        step4_response = self.call_llm_with_history(model_name, image_path, step4_messages)
        
        if not step4_response:
            print("âŒ Step 4 (Answer Extraction) ì‹¤íŒ¨")
            return
        
        print("âœ… ìµœì¢… ë‹µë³€:")
        print(step4_response)
        
        # ìµœì¢… ê²°ê³¼ íŒŒì¼ ì €ì¥
        self.save_final_results(
            model_name, scenario, image_name,
            step1_response, user_info_text, 
            reasoning_response, step4_response
        )
        
        print("ğŸ‰ ì²˜ë¦¬ ì™„ë£Œ!")

    def process_all_images(self):
        """ëª¨ë“  ì´ë¯¸ì§€ ì²˜ë¦¬"""
        print("ğŸ¯ ì§ ë¶€í”¼ ì¶”ì • ì‹œìŠ¤í…œ ì‹œì‘...")
        
        # í”„ë¡¬í”„íŠ¸ íŒŒì¼ ì¡´ì¬ í™•ì¸
        required_files = ["zero_chat1.txt", "zero_chat2.txt"]
        for filename in required_files:
            file_path = self.prompt_path / filename
            if not file_path.exists():
                raise FileNotFoundError(f"í•„ìˆ˜ í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        
        for model_name in self.models:
            print(f"\n{'='*100}")
            print(f"ëª¨ë¸: {model_name} ì²˜ë¦¬ ì‹œì‘")
            print(f"{'='*100}")
            
            for scenario in self.scenarios:
                print(f"\nì‹œë‚˜ë¦¬ì˜¤: {scenario}")
                
                for image_name in ["1", "2"]:
                    try:
                        self.process_single_image(model_name, scenario, image_name)
                        
                        # ë‹¤ìŒ ì´ë¯¸ì§€ ì²˜ë¦¬ ì „ í™•ì¸
                        user_input = input("\në‹¤ìŒ ì´ë¯¸ì§€ë¡œ ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (Enter: ê³„ì†, q: ì¢…ë£Œ): ")
                        if user_input.lower() == 'q':
                            print("â¹ï¸ ì²˜ë¦¬ ì¤‘ë‹¨ë¨")
                            return
                            
                    except KeyboardInterrupt:
                        print("\n\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
                        return
                    except Exception as e:
                        print(f"âŒ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨ ({model_name}/{scenario}/{image_name}): {e}")
                        continue
        
        print("\n" + "="*100)
        print("ğŸ ëª¨ë“  ì²˜ë¦¬ ì™„ë£Œ!")
        print("="*100)

# ì‹¤í–‰ ì½”ë“œ
if __name__ == "__main__":
    estimator = LuggageVolumeEstimator()
    estimator.process_all_images()