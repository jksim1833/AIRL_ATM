from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import JsonOutputParser
from langchain_community.callbacks import get_openai_callback
import tiktoken
import json
import os
import re
import argparse
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# í† í¬ë‚˜ì´ì € ì„¤ì •
enc = tiktoken.get_encoding("cl100k_base")

# ë””ë ‰í† ë¦¬ ì„¤ì • (langchain_v1 í´ë” ë‚´ì˜ ê²½ë¡œ)
dir_system = './langchain_v1/chain3_system'
dir_prompt = './langchain_v1/chain3_prompt'
dir_query = './langchain_v1/chain3_query'
prompt_load_order = ['chain3_prompt_role',
                     'chain3_prompt_function',
                     'chain3_prompt_environment',
                     'chain3_prompt_output_format',
                     'chain3_prompt_example']


class LangChainChatGPT:
    def __init__(self, prompt_load_order):
        # LangChain ChatOpenAI ëª¨ë¸ ì´ˆê¸°í™”
        self.llm = ChatOpenAI(
            model="gpt-4o",
            temperature=0.1,
            max_tokens=1000,
            top_p=0.5,
            frequency_penalty=0.0,
            presence_penalty=0.0
        )
        
        # JSON ì¶œë ¥ íŒŒì„œ ì„¤ì •
        self.output_parser = JsonOutputParser()
        
        self.messages = []
        self.max_token_length = 10000
        self.max_completion_length = 1000
        self.last_response = None
        self.query = ''
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ
        fp_system = os.path.join(dir_system, 'chain3_system.txt')
        with open(fp_system, encoding='utf-8') as f:
            system_content = f.read()
        self.system_message = SystemMessage(content=system_content)
        
        # í”„ë¡¬í”„íŠ¸ íŒŒì¼ë“¤ ë¡œë“œ
        for prompt_name in prompt_load_order:
            fp_prompt = os.path.join(dir_prompt, prompt_name + '.txt')
            with open(fp_prompt, encoding='utf-8') as f:
                data = f.read()
            
            # [user], [assistant] íƒœê·¸ë¡œ ë¶„í• 
            data_split = re.split(r'\[user\]\n|\[assistant\]\n', data)
            data_split = [item for item in data_split if len(item) != 0]
            
            assert len(data_split) % 2 == 0
            for i, item in enumerate(data_split):
                if i % 2 == 0:
                    self.messages.append(HumanMessage(content=item))
                else:
                    self.messages.append(AIMessage(content=item))
        
        # ì¿¼ë¦¬ íŒŒì¼ ë¡œë“œ
        fp_query = os.path.join(dir_query, 'chain3_query.txt')
        with open(fp_query, encoding='utf-8') as f:
            self.query = f.read()
    
    def create_prompt_template(self):
        """ChatPromptTemplate ìƒì„±"""
        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ì™€ ëŒ€í™” íˆìŠ¤í† ë¦¬, ê·¸ë¦¬ê³  í˜„ì¬ ì§ˆë¬¸ì„ í¬í•¨í•œ í…œí”Œë¦¿
        template = ChatPromptTemplate.from_messages([
            self.system_message,
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{query}")
        ])
        return template
    
    def calculate_token_length(self, messages):
        """ë©”ì‹œì§€ë“¤ì˜ í† í° ê¸¸ì´ ê³„ì‚°"""
        total_content = ""
        for message in messages:
            if hasattr(message, 'content'):
                total_content += message.content
        return len(enc.encode(total_content))
    
    def truncate_messages_if_needed(self):
        """í† í° ê¸¸ì´ê°€ ì´ˆê³¼ë˜ë©´ ì˜¤ë˜ëœ ë©”ì‹œì§€ ì œê±°"""
        current_length = self.calculate_token_length(self.messages)
        print(f'prompt length: {current_length}')
        
        while current_length > self.max_token_length - self.max_completion_length:
            if len(self.messages) >= 2:
                print('prompt too long. truncated.')
                # ê°€ì¥ ì˜¤ë˜ëœ ë‘ ë©”ì‹œì§€ ì œê±° (user-assistant ìŒ)
                self.messages = self.messages[2:]
                current_length = self.calculate_token_length(self.messages)
            else:
                break
    
    def extract_json_part(self, text):
        """GPT ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ"""
        if text.find('```') == -1:
            return text
        
        # ì²« ë²ˆì§¸ì™€ ë‘ ë²ˆì§¸ ``` ì‚¬ì´ì˜ ë‚´ìš© ì¶”ì¶œ
        start_idx = text.find('```') + 3
        end_idx = text.find('```', start_idx)
        if end_idx != -1:
            text_json = text[start_idx:end_idx]
        else:
            text_json = text[start_idx:]
        
        return text_json.strip()
    
    def generate(self, message, environment):
        """LangChainì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±"""
        # ì¿¼ë¦¬ í…œí”Œë¦¿ì— í™˜ê²½ê³¼ ëª…ë ¹ì–´ ì‚½ì…
        text_base = self.query
        if text_base.find('[ENVIRONMENT]') != -1:
            text_base = text_base.replace('[ENVIRONMENT]', json.dumps(environment))
        
        if text_base.find('[INSTRUCTION]') != -1:
            text_base = text_base.replace('[INSTRUCTION]', message)
        
        # í† í° ê¸¸ì´ í™•ì¸ ë° ì¡°ì •
        self.truncate_messages_if_needed()
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
        prompt_template = self.create_prompt_template()
        
        # LLM ì²´ì¸ ìƒì„±
        chain = prompt_template | self.llm | self.output_parser
        
        try:
            # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì ê³¼ í•¨ê»˜ ì‹¤í–‰
            with get_openai_callback() as cb:
                # ì‘ë‹µ ìƒì„±
                response = chain.invoke({
                    "chat_history": self.messages,
                    "query": text_base
                })
                
                print(f"Total Tokens: {cb.total_tokens}")
                print(f"Prompt Tokens: {cb.prompt_tokens}")
                print(f"Completion Tokens: {cb.completion_tokens}")
                print(f"Total Cost (USD): ${cb.total_cost}")
        
        except Exception as e:
            print(f"JSON íŒŒì‹± ì‹¤íŒ¨, í…ìŠ¤íŠ¸ ì‘ë‹µìœ¼ë¡œ ì²˜ë¦¬: {e}")
            # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ ì²´ì¸ ì‚¬ìš©
            text_chain = prompt_template | self.llm
            response_message = text_chain.invoke({
                "chat_history": self.messages,
                "query": text_base
            })
            
            text = response_message.content
            print(text)
            
            # JSON ë¶€ë¶„ ì¶”ì¶œ ë° ì²˜ë¦¬
            self.last_response = self.extract_json_part(text)
            self.last_response = self.last_response.replace("'", "\"")
            
            # ì‘ë‹µì„ íŒŒì¼ë¡œ ì €ì¥
            with open('chain3_last_response.txt', 'w', encoding='utf-8') as f:
                f.write(self.last_response)
            
            try:
                response = json.loads(self.last_response, strict=False)
            except json.JSONDecodeError as json_error:
                print(f"JSON íŒŒì‹± ì‹¤íŒ¨: {json_error}")
                import pdb
                pdb.set_trace()
                return None
        
        # ì‘ë‹µ ì €ì¥ ë° ì²˜ë¦¬
        self.last_response = json.dumps(response, ensure_ascii=False, indent=2)
        
        # ì‘ë‹µì„ íŒŒì¼ë¡œ ì €ì¥
        with open('chain3_last_response.txt', 'w', encoding='utf-8') as f:
            f.write(self.last_response)
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        self.messages.append(HumanMessage(content=text_base))
        self.messages.append(AIMessage(content=self.last_response))
        
        # í™˜ê²½ ì—…ë°ì´íŠ¸
        if "environment_after" in response:
            self.environment = response["environment_after"]
        
        return response
    
    def dump_json(self, dump_name=None):
        """JSON ì‘ë‹µì„ íŒŒì¼ë¡œ ì €ì¥"""
        if dump_name is not None and self.last_response is not None:
            fp = os.path.join(dump_name + '.json')
            with open(fp, 'w', encoding='utf-8') as f:
                if isinstance(self.last_response, str):
                    # ë¬¸ìì—´ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì €ì¥
                    f.write(self.last_response)
                else:
                    # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° JSONìœ¼ë¡œ ë³€í™˜
                    json.dump(self.last_response, f, indent=4, ensure_ascii=False)


# ì‹œë‚˜ë¦¬ì˜¤ ì„ íƒ ë° ì‹¤í–‰
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--scenario',
        type=str,
        required=True,
        help='scenario name (see the code for details)')
    args = parser.parse_args()
    scenario_name = args.scenario

    # ì‹œë‚˜ë¦¬ì˜¤ë³„ í™˜ê²½ ë° ëª…ë ¹ì–´ ì„¤ì •
    if scenario_name == 'chain3_1seat':
        environment = {
            "objects": ["seat_1"],
            "object_states": {
                "seat_1": {
                    "type": "seat",
                    "position": [1, 1],
                    "direction": "A"
                }
            }
        }
        instructions = [
            "seat_1(storage, 1, 1, A)",
            "seat_1(storage, 1, 1, C)",
            "seat_1(seat, 2, 2, C)",
            "seat_1(storage, 0, 0, B)"
        ]
        
    elif scenario_name == 'chain3_4seat_compact':
        environment = {
            "objects": ["seat_1", "seat_2", "seat_3", "seat_4"],
            "object_states": {
                "seat_1": {"type": "seat", "position": [1, 1], "direction": "A"},
                "seat_2": {"type": "seat", "position": [1, 1], "direction": "A"},
                "seat_3": {"type": "seat", "position": [1, 1], "direction": "A"},
                "seat_4": {"type": "seat", "position": [1, 1], "direction": "A"}
            }
        }
        instructions = [
            "seat_1(storage, 0, 0, B), seat_2(storage, 2, 0, D), seat_3(storage, 0, 0, B), seat_4(storage, 2, 0, D)"
        ]
        
    elif scenario_name == 'chain3_4seat_pair_face':
        environment = {
            "objects": ["seat_1", "seat_2", "seat_3", "seat_4"],
            "object_states": {
                "seat_1": {"type": "storage", "position": [1, 0], "direction": "C"},
                "seat_2": {"type": "storage", "position": [1, 0], "direction": "C"},
                "seat_3": {"type": "storage", "position": [0, 1], "direction": "B"},
                "seat_4": {"type": "storage", "position": [2, 1], "direction": "D"}
            }
        }
        instructions = [
            "seat_1(seat, 1, 2, B), seat_2(seat, 1, 2, D), seat_3(seat, 1, 0, B), seat_4(seat, 1, 0, D)"
        ]
        
    else:
        parser.error('Invalid scenario name: ' + scenario_name)

    # AI ëª¨ë¸ ì´ˆê¸°í™”
    aimodel = LangChainChatGPT(prompt_load_order=prompt_load_order)

    # ì¶œë ¥ ê²°ê³¼ ì €ì¥ í´ë” ìƒì„± (langchain_v1 í´ë” ë‚´ì—)
    if not os.path.exists('./langchain_v1/chain3_out/' + scenario_name):
        os.makedirs('./langchain_v1/chain3_out/' + scenario_name)

    # ê° ëª…ë ¹ì–´ì— ëŒ€í•´ ì²˜ë¦¬
    for i, instruction in enumerate(instructions):
        print(f"\n=== Instruction {i+1} ===")
        print(json.dumps(environment, indent=2, ensure_ascii=False))
        
        # GPTì—ê²Œ instructionê³¼ environment ì „ë‹¬í•˜ì—¬ ì‘ë‹µ ìƒì„±
        response = aimodel.generate(instruction, environment)
        
        if response and "environment_after" in response:
            # í™˜ê²½ ìƒíƒœ ì—…ë°ì´íŠ¸
            environment = response["environment_after"]
            
            # ì‘ë‹µì„ JSON íŒŒì¼ë¡œ ì €ì¥ (langchain_v1 í´ë” ë‚´ì—)
            aimodel.dump_json(f'./langchain_v1/chain3_out/{scenario_name}/{i}')
            
            print(f"âœ… Step {i+1} completed successfully")
        else:
            print(f"âŒ Step {i+1} failed")
            break

    print(f"\nğŸ‰ Scenario '{scenario_name}' completed!")